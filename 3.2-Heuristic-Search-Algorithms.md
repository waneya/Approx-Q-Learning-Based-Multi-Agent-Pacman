# Approach 2: Heuristic search algorithms

Heuristic search algorithms are those that attempt to estimate the cost of reaching a certain goal, given a partial step towards that goal. In the context of pacman this can be very helpful for two reasons:
1. Some search algorithms that provide optimal solutions (e.g. minimum spanning tree) are computationally expensive and unfeasible within the 1sec inter-turn computational time limit for each agent. Heuristic search algorithms can overcome this and still at times provide an equally optimal solution if they are admissible; and
2. There are some problems that must be tackled to improve agent performance, which are not directly informed to us by the game, and so, a heuristic is created to find a sufficient method of approaching that problem - and heuristic functions are utilised to contribute to the action decision process of the agents (after appropriate weighting).

There are certain features utilised within our approximate Q-learning approach that require costly search calculations to achieve a true value representation. And so, heuristic functions can act as reasonable proxies to approximate values. However this must be done carefully as ill-defined (i.e. inadmissible) heuristics could lead to sub-optimal and adverse agent actions taken.

### 'Dead-end' heuristic and 7-step iterative deepening search

To tackle the issue of dead-ends, and our agents being trapped by them in enemy territory, first we decided on a heuristic for solving this problem, which was how many possible actions could an agent take at a given state (i.e. 4 - # surround walls; not including 'stop' action). This combined with an iterative deepening search allowed our agents to reasonably avoid dead ends when escaping enemy ghosts.

A challenge of this approach remained in terms of the computational expense of calculating this at every turn. We found that our team failed the time limit when searches were completed in anticipation of 8+ steps, and so we settled for our agents to account for dead-ends within a 7-step understanding. That is, our agents took into account which action would lead to the greatest sum of possible actions over 7 future actions (not including moving back to a previous position) - in order to avoid dead ends.

### A two-tiered heuristic for distance to agents

When agents are directly visible within 5 manhattan distance, we utilised maze distance to get the perfect heuristic of minimal distance to an agent. We were also provided with a noisy distance that approximated an agent's position outside of the visible range, however this does not let our agents know the direction in which the respective agent's noisy distance is. When an agent eats a food pellet, it is possible to find their exact position and minimal distance through maze distance once again - the second tier heuristic evaluation. Our 'Goal Recognition' section goes into depth of this mechanism, but is included here to outline the heuristic mechanism of maze distance and food being eaten.

### Food groups; searching for 'high food' areas

A question of how to prioritise which food to chase in enemy territory is important if wishing to maximise score over time and chances of winning. For example, certain areas contain higher amounts of food than others, but the method to classify groups of food is not so obvious. We utilised a heuristic that classified groups as those which contained food which were directly adjacent to each other, which acted as a admissible heuristic in itself. Holding all else constant, this should help our agents prioritise areas with greater amounts of food to eat more efficiently in terms of time.

### A heuristic for the risk trade-off of eating more food or going home

A question arises of when should an agent return home after eating food? This is potentially a difficult question from the context of pacman. We utilised a heuristic, being sqrt(height*width) to gauge the likelihood of running into enemy agents. For example, in the case of an offensive agent, if it knows that this value is low, then the likelihood of running into an enemy defensive agent and getting eaten is higher.

### Challenges experienced


### Possible improvements  



[Previous Page](/2_1_approach) | [Next Page](/2_3_approach)