# Approach 2: Heuristic search algorithms

Heuristic search algorithms are those that attempt to estimate the cost of reaching a certain goal, given a partial step towards that goal. In the context of pacman this can be very helpful for two reasons:
1. Some search algorithms that provide optimal solutions (e.g. minimum spanning tree) are computationally expensive and unfeasible within the 1sec inter-turn computational time limit for each agent. Heuristic search algorithms can overcome this and still at times provide an equally optimal solution if they are admissible; and
2. There are some problems that must be tackled to improve agent performance, which are not directly informed to us by the game, and so, a heuristic is created to find a sufficient method of approaching that problem - and heuristic functions are utilised to contribute to the action decision process of the agents (after appropriate weighting).

There are certain features utilised within our approximate Q-learning approach that require costly search calculations to achieve a true value representation. And so, heuristic functions can act as reasonable proxies to approximate values. However this must be done carefully as ill-defined (i.e. inadmissible) heuristics could lead to sub-optimal and adverse agent actions taken.

In this section, we discuss and explore 4 heuristic approaches which we either implemented in our final version or attempted.

# 'Dead-end' heuristic and 7-step iterative deepening search

To tackle the issue of dead-ends, and our agents being trapped by them in enemy territory, first we decided on a heuristic for solving this problem, which was how many possible actions could an agent take at a given state (i.e. 4 - # surround walls; not including 'stop' action). This combined with an iterative deepening search allowed our agents to reasonably avoid dead ends when escaping enemy ghosts.

A challenge of this approach remained in terms of the computational expense of calculating this at every turn. We found that our team failed the time limit when searches were completed in anticipation of 8+ steps, and so we settled for our agents to account for dead-ends within a 7-step understanding. That is, our agents took into account which action would lead to the greatest sum of possible actions over 7 future actions (not including moving back to a previous position) - in order to avoid dead ends.

# A two-tiered heuristic for distance to agents

When agents are directly visible within 5 manhattan distance, we utilised maze distance to get the perfect heuristic of minimal distance to an agent. We were also provided with a noisy distance that approximated an agent's position outside of the visible range, however this does not let our agents know the direction in which the respective agent's noisy distance is. When an agent eats a food pellet, it is possible to find their exact position and minimal distance through maze distance once again - the second tier heuristic evaluation. Our 'Goal Recognition' section goes into depth of this mechanism, but is included here to outline the heuristic mechanism of maze distance and food being eaten.

# Classifying 'food groups' heuristic; searching for 'high food' areas

A question of how to prioritise which food to chase in enemy territory is important if wishing to maximise score over time and chances of winning. For example, certain areas contain higher amounts of food than others, but the method to classify groups of food is not so obvious. 

We utilised a heuristic that classified food groups as those which contained food which were directly adjacent to each other, which acted as a admissible heuristic in itself. And a food group which had a higher number of adjacent foods was deemed more attractive to maximise score than those with lower amounts of adjacent food. Holding all else constant, this should help our agents prioritise areas with greater amounts of food to eat more efficiently in terms of time.

# A heuristic for the risk of running into enemy agents

A question arises of when should an agent return home after eating food? This is potentially a difficult question from the context of pacman. We utilised a heuristic, being sqrt(height*width) of the map being played to gauge the likelihood of running into enemy agents. For example, in the case of an offensive agent, if it knows that this value is low, then the likelihood of running into an enemy defensive agent and getting eaten is higher. This heuristic can be combined with other features, such as how much food is the agent currently holding, or the time left in the game as components which influence whether an agent should decide to head home to maximise score & game winning likelihood.

This heuristic is not always effective in overcoming its intended problem, and also depends on the enemy agents' strategy, so it was difficult to test its efficacy in an empirical sense. If more competition sampling was possible, then we could attempt to isolate its impact. By this same reasoning, this heuristic is not necessarily admissible either, because even though a map may be large, it may be more likely for agents to run into each other than e.g. a small map, depending on the grid layout, wall structures, agent behaviours and other unforeseen variables.

### Challenges experienced for all heuristics
* Computational challenges still remained, such as in the 'dead-end' heuristic method.
* Some heuristics, such as our food groups were sensibly admissible, but lose relevant information, because for example, to remain admissible, it does not account for food that is not adjacent but could e.g. be only 1 additional step away.
* It was difficult to create heuristics and test their effectiveness, due to lack of full sampling methods (e.g. limited testing times against non-independent agent behaviours through the nightly competition).

### Possible improvements for all heuristics
* Given more time and less pressure, we would systematically test the efficacy of each heuristic - attempting to isolate their impact as much as possible. And in addition, ensure that they are working as intended or at least within admissible impact. Our current methodology included adding a few at a time, or possibly requiring debugging along the way which made it difficult to achieve a maximally rigorous testing process.
* Systematic testing could be achieved through our Q-learning approach if we had access to a greater variety of offline agent behaviours to freely sample. 
* We did not have time to explore cooperative behaviour between our allied agents, for which heuristics could be quite effective. For example, if two of our agents are defneding, utilising the food group mechanism and an understanding of an ally agent's position in an attempt to maximise total food being within an effective defensive range.


[Previous Page](/2_1_approach) | [Next Page](/2_3_approach)